# 인공지능의 학습
- 다변수 함수의 최적화 문제로, 최적화를 위해서 각각의 변수에 대해 미분을 하여 최소점을 구해야 한다.

# 시그모이드 함수: 단점
- 음수 부분을 표현하지 못하는 문제: 시그모이드는 0에서 1 사이의 수만 출력.
- 기울기 소실 문제: 시그모이드 함수를 여러 개의 은닉층을 가진 딥러닝에 적용하게 되면, 0.25가 계속 곱해져 점점 0에 가까워지고, 이는 기울기가 사라져 더 이상 학습을 하지 못하는 문제를 발생.

# 활성화 함수
1) softmax
- 0~1사이의 확률값으로 출력(분류 문제에서 많이 사용)한다.
- argmax 명령어를 이용해 가장 큰 값을 출력하는 클래스를 정답으로 추정한다.
- softmax에서 하나의 입력을 0으로 강제한 2-class softmax 함수는 sigmoid와 동일하다.
![Image](https://github.com/user-attachments/assets/ade7fe5e-408e-4706-9bed-68dc07ad6b6c)

2) tanh
- "-1 ~ 1"까지를 출력, 연속이며 미분이 쉽다. (회귀문제, LSTM 등에서 많이 사용)

3) ReLU
- 0 미만은 0, 0 이상은 자기 자신을 출력(이미지 학습용 모델인 CNN에서 많이 사용)한다.

4) sigmoid
- "0~1"까지를 출력, 연속이며 미분이 쉽다(회귀 문제에서 많이 사용).

# 인공신경망의 작동 프로세스
1) 임의의 가중치와 기준치를 가지고 신경망 위에서 계산함.
2) 계산되어 나온 출력과 정답을 비교하여 손실함수(오차함수)를 계산함.
3) 오차를 최소화하기 위해 가중치와 기준치 값을 조절함.
4) 오차가 0에 가까워지거나, 더이상 기준치, 가중치 값이 변하지 않으면 학습을 종료함.
5) 학습이 종료되면 더 이상 정답을 가르쳐 주지 않아도 올바른 출력값을 계산해 냄.
