# 인공지능의 학습
- 다변수 함수의 최적화 문제로, 최적화를 위해서 각각의 변수에 대해 미분을 하여 최소점을 구해야 한다.

# 시그모이드 함수: 단점
- 음수 부분을 표현하지 못하는 문제: 시그모이드는 0에서 1 사이의 수만 출력.
- 기울기 소실 문제: 시그모이드 함수를 여러 개의 은닉층을 가진 딥러닝에 적용하게 되면, 0.25가 계속 곱해져 점점 0에 가까워지고, 이는 기울기가 사라져 더 이상 학습을 하지 못하는 문제를 발생.

# 활성화 함수
1) softmax
- 0~1사이의 확률값으로 출력(분류 문제에서 많이 사용)한다.
- argmax 명령어를 이용해 가장 큰 값을 출력하는 클래스를 정답으로 추정한다.
- softmax에서 하나의 입력을 0으로 강제한 2-class softmax 함수는 sigmoid와 동일하다.
![Image](https://github.com/user-attachments/assets/ade7fe5e-408e-4706-9bed-68dc07ad6b6c)

2) tanh
- "-1 ~ 1"까지를 출력, 연속이며 미분이 쉽다. (회귀문제, LSTM 등에서 많이 사용)

3) ReLU
- 0 미만은 0, 0 이상은 자기 자신을 출력(이미지 학습용 모델인 CNN에서 많이 사용)한다.

4) sigmoid
- "0~1"까지를 출력, 연속이며 미분이 쉽다(회귀 문제에서 많이 사용).
